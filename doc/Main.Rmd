---
title: "US Inaugural Speeches - Focus on presidents having done two terms in office"
subtitle: "What is the mindset of a reelected president? Is there any trend or pattern that can be identified among reelected presidents?"
runtime: shiny
author: "Marie Schiltz"
output: html_document
---

This is an *Interactive* [R Markdown](http://rmarkdown.rstudio.com) Notebook. It generates an HTML notebook that would allow users to interactively explore your analysis results. 

This analysis is based on the inaugural speeches of all the US presidents since George Washington in 1789.  
With this study, I would like to determine what is the mindset of a president beginning his second term in office. Is there any difference with a president beginning his first term? How a president appointed for the second time (and the last one) begin his new term?  For this reason, the study will focus on presidents who have done two terms in office.  
I will try to answer the following questions:  
* Is the president more confident during the second inaugural speech?  
* Are the subjects of his speech the same?  
* Is he more optimistic or more realistic?   
Of course, I'll adapt my study depending on what I find during the process.

[Note] Don't forget to download the entire folder provided as each subfolder is important. 

## Step 0 - Install and load libraries
```{r, message=FALSE, warning=FALSE}

packages.used=c("tidytext", "dplyr", "tm", "ggplot2", "wordcloud", "rvest", 
                "tibble", "qdap", "sentimentr", "gplots", "syuzhet", 
                "factoextra", "beeswarm", "scales", "RColorBrewer", "RANN", 
                "topicmodels", "splitstackshape", "tidyr", "shiny", "gridExtra")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, intersect(installed.packages()[,1], 
                                                 packages.used))

# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

# load packages
library("tidytext")
library("dplyr")
library("tm")
library("ggplot2")
library("wordcloud")
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("splitstackshape")
library("tidyr")
library("shiny")
library("gridExtra")

# source functions
source("../lib/processing.R")
source("../lib/displaying.R")
```

This notebook was prepared with the following environmental settings.

```{r}
print(R.version)
```

## Step 1 - Read in the speeches
```{r}
 
folder.path = "../data/InauguralSpeeches/"
speeches = list.files(path = folder.path, pattern = "*.txt")
ff.all <- Corpus(DirSource(folder.path))

```

## Step 2 - Text processing

For the speeches, we remove extra white space, convert all letters to the lower case, remove [stop words](https://github.com/arc12/Text-Mining-Weak-Signals/wiki/Standard-set-of-english-stopwords), removed empty words due to formatting errors, and remove punctuation. Then we compute the [Document-Term Matrix (DTM)](https://en.wikipedia.org/wiki/Document-term_matrix). 

```{r}

# remove white Spaces, transform to lower case, remove stop words, remove empty words and remove punctuation
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)

# compute Term-Document Matrix
tdm <- TermDocumentMatrix(ff.all)

# compute Document-Term Matrix
dtm <- DocumentTermMatrix(ff.all)
dtm.tidy <- tidy(dtm)
dtm.all <- summarise(group_by(dtm.tidy, term), sum(count))

# # inspect dtm or tdm
# inspect(tdm[100:105,25:30])
# inspect(dtm[25:27,100:104])
# dim(dtm.idf)

#Note: I am using both tdm and dtm, because it's easier to use one or the other for some applications later on.
```

## Step 3 - compute TF-IDF weighted document-term matrices for individual speeches. 
As we would like to identify interesting words for each inaugural speech, we use [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to weigh each term within each speech. It highlights terms that are more specific for a particular speech. 

```{r}

# Compute TermDocument Matrix weighted with TF-IDF principle
tdm.idf <- TermDocumentMatrix(ff.all,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, normalize =FALSE),
                                         stopwords = TRUE))

# Compute DocumentTerm Matrix weighted with TF-IDF principle
dtm.idf <- DocumentTermMatrix(ff.all,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, normalize =FALSE),
                                         stopwords = TRUE))
dtm.idf.tidy <- tidy(dtm.idf)
dtm.idf.all <- summarise(group_by(dtm.idf.tidy, term), sum(count))

# # Cast into a Matrix object
# m.tdm.idf.tidy <- tdm.idf.tidy %>%
#   cast_sparse(document, term, count)

```

## Step 4 - Identify presidents who did two terms in office

As discussed earlier, this study will focus on presidents who've spent two terms in office. For this reason I will remove from the corpus all other presidents. President Lincoln who did two terms in office is to be kept in this analysis.

Below you'll find the list of presidents who did two terms in office.

```{r}

# list of presidents who did two terms in office
prex.out <- substr(speeches, 6, nchar(speeches)-4)
speeches.info <- as.data.frame(prex.out)
speeches.info <- cSplit(speeches.info, "prex.out", "-")
colnames(speeches.info)[1] <- "name"
colnames(speeches.info)[2] <- "term"
speeches.info$prex.out_3 <- NULL

president.list <- as.character(speeches.info[speeches.info$term==2,]$name)
print(president.list)

```

## Step 5 - Data processing: Update tables to keep only presidents who were elected twice

```{r}

# update all dtms tables so that they contain only speeches given by presidents who stayed two terms in office
# see the file processing.R for details about the functions used in this chunk
dtm.tidy <- update.tables(dtm.tidy,president.list)
dtm.idf.tidy <- update.tables(dtm.idf.tidy,president.list)

dtm.all <- summarise(group_by(dtm.tidy, word), count = sum(count))
dtm.idf.all <- summarise(group_by(dtm.idf.tidy, word), count = sum(count))

```

## Step 6 - Word Frequency - Wordcloud

This section is the first visualisation section, I propose you to have a look at some wordclouds. Each time, you will have on the left the wordcloud corresponding on the first inaugural speech and on the right the wordcloud corresponding to the second inaugural speech of the selected president.

```{r, echo= FALSE, warning = FALSE, fig.height = 18}

ui <- fluidPage(selectInput(inputId = "data","", president.list), 
                plotOutput(outputId = "wordclouds")
)

server <- function(input, output) {
  data1 <- reactive({
        dtm.idf.tidy[dtm.idf.tidy$document==paste0(input$data,"-1"),]
      })
  data2 <- reactive({
        dtm.idf.tidy[dtm.idf.tidy$document==paste0(input$data,"-2"),]
      })
    
  output$wordclouds <- renderPlot({
    layout(matrix(c(1,3,2,4), nrow=2, ncol=2), heights=c(0.5, 4))
    #layout.show(4)
    par(mar=c(1,0,0,0)) #margins
    plot.new()
    text(x=0.5, y=0.5, cex=2, "1st Term", font=4)
    plot.new()
    text(x=0.5, y=0.5, cex=2, "2nd Term", font=4)
    wordcloud(data1()$word, data1()$count,
            main = "1st Term",
            scale=c(4,0.5),
            max.words=70,
            min.freq=1,
            random.order=FALSE,
            rot.per=0.0,
            use.r.layout=T,
            random.color=FALSE,
            colors=brewer.pal(9,"Blues"))
    wordcloud(data2()$word, data2()$count,
            main = "2nd Term",
            scale=c(4,0.5),
            max.words=70,
            min.freq=1,
            random.order=FALSE,
            rot.per=0.0,
            use.r.layout=T,
            random.color=FALSE,
            colors=brewer.pal(9,"Greens"))
  }) 
}

shinyApp(ui = ui, server = server, options = list(height = 600))

# wordcloud.comparison(dtm.idf.tidy, "BarackObama")
# wordcloud.comparison(dtm.idf.tidy, "GeorgeWBush")
# wordcloud.comparison(dtm.idf.tidy, "RonaldReagan")

```

From those wordclouds, one can already identify important topics and differences between the first and second inaugural speeches of each president.

Barack Obama, first speech seems focused on natural catastrophes and employment, while his second speech seems more philosophical (words like "journey" or creed).

In William Clinton speeches, we also find this transition to a second speech talking about a journey and the passing of time ("century").  

Of course, this is not true for all presidents, Reagan speeches use similar lexicons (repetition of the word "weapon")

However, a wordcloud doesn't allow us to have an overall appreciation of the speech. One would like to know the overall atmosphere - and to be able to know if the speech is optimistic or not. To do that we will focus on sentiment analysis in the next section.

## Step 7: Sentiment Analysis - Overall score

For each word we will apply sentiment analysis using [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). “The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing.”

Note that the tidytext package also contains the sentiments dataset which includes three useful lexicons:
- nrc: the one we are using
- afinn: assigns a score to each word (the score is between -5 and 5, negative scores indicate negative sentiments)
- bing: categorizes words into positive and negative categories
One can access those lexicons by using the function get_sentiment()

The goal of this section is to compute an overall score for each speech. Each negative word will be counted negatively and weighted depending on its redundancy - the same will be applied for positive words.

```{r, fig.align="center"}
# get sentiments
dtm.idf.sentiment <- inner_join(dtm.idf.tidy,
                                get_sentiments("nrc"),
                                by=c("word"="word"))

# spread sentiments in different columns
dtm.idf.sentiment <- spread(dtm.idf.sentiment, sentiment, count, fill=0)

# compute a sentiment score for each speech
sentiment.score <- mutate(dtm.idf.sentiment, feeling = positive - negative)
sentiment.score <- summarise(group_by(sentiment.score, document, president, term), 
                             score = sum(feeling))

c <- ggplot(sentiment.score, aes(x=sentiment.score$president, 
                                 y=sentiment.score$score, 
                                 fill=sentiment.score$term)) + 
  geom_bar(stat="identity", position = "dodge", width=.6) +
  ylab("Sentiment Score") +
  xlab("US Presidents") +
  scale_fill_manual("Term", values=brewer.pal(9,"Set1")) +
  theme(axis.text.x = element_text(angle = 45, size=12, hjust = 1))
c
```

The first output we can draw from this graph is that approximately two third of US presidents got similar scores for the two speeches (Wilson, Grant, Nixon, Bush, Obama). This could be due to similar speeches or lexicons. 
What would be interesting is to know if they began their second term with a more optimistic (positive lexicon) or a more pessimistic mindset.
 
## Step 8 - Sentiment Analysis - Difference between first speech score and second speech score

```{r, fig.align="center"}

# compute difference between score of speech one and score of speech two
sentiment.score.shift = as.data.frame(matrix(ncol=2, nrow=length(president.list)))
names(sentiment.score.shift)[names(sentiment.score.shift)=="V1"] <- "president"
names(sentiment.score.shift)[names(sentiment.score.shift)=="V2"] <- "score"

for (i in 1:length(president.list)){
  a <- sentiment.score[(sentiment.score$president == 
                         president.list[i] & sentiment.score$term == "1"),"score"]
  b <- sentiment.score[sentiment.score$president == 
                         president.list[i] & sentiment.score$term == "2","score"]
  sentiment.score.shift$score[i] <- -(a-b)
  sentiment.score.shift$president[i] <- president.list[i]
}

# assigne colour depending on the sign of the score
sentiment.score.shift$colour <- ifelse(sentiment.score.shift$score < 0, 
                                       "negative","positive")
sentiment.score.shift$score <- as.numeric(sentiment.score.shift$score)

d <- ggplot(sentiment.score.shift, aes(x=sentiment.score.shift$president, 
                                       y=sentiment.score.shift$score)) + 
  geom_bar(stat="identity", width=.6, aes(fill = colour)) +
  ylab("Sentiment Score Shift") +
  xlab("US Presidents") +
  scale_fill_manual("Term", values=c(positive="steelblue",negative="firebrick1")) + 
  theme(axis.text.x = element_text(angle = 45, size=12, hjust = 1))
d

```

Most presidents gave a more negative speech at their second inauguration. What is really important to notice from this graph is the value of the negative shifts compared to those of the positive shifts. 

## Step 9 - What are the words impacting those scores? 

```{r, warning=FALSE, message=FALSE, fig.align="center"}
# selecting positive words
word.score.positive <- summarise(group_by(dtm.idf.sentiment, word), score=sum(positive))
word.score.positive <- arrange(word.score.positive, desc(score))

# plotting
word.score.positive %>%
  arrange(score) %>%
  mutate(word = factor(word, word)) %>%
  top_n(15) %>%
  ggplot(aes(x=word, y=score)) + 
    geom_bar(stat="identity", fill="#FF9999") +
    xlab("Positive Words") +
    ggtitle("Positive words impacting the overall score") +
    coord_flip()
```

The words "democracy" and "freedom" are widely used in all speeches and contribute to positive scores along with their lexicons ("peace", "equally").

```{r, warning=FALSE, message=FALSE, fig.align="center"}
# selecting negative words
word.score.negative <- summarise(group_by(dtm.idf.sentiment, word), score=sum(negative))
word.score.negative <- arrange(word.score.negative, desc(score))

# plotting
word.score.negative %>%
  arrange(score) %>%
  mutate(word = factor(word, word)) %>%
  top_n(15) %>%
  ggplot(aes(x=word, y=score)) + 
    geom_bar(stat="identity", fill="#999999") +
    xlab("Negative Words") +
    ggtitle("Negative words impacting the overall score") +
    coord_flip()
```

Among with negative words the conflict's lexicon is the more represented ("conflict", "war"", "force", "enemy", "invasion", "force").
One possibility is that US presidents are more pessimistic about world conflicts after one term in office. At this stage of the analysis it's only a hypothesis. 
Let's compare negative words used by US presidents in their first and their second speeches.

```{r, warning=FALSE, fig.align="center"}
# selecting negative words by term
word.negative.term <- summarise(group_by(dtm.idf.sentiment, word, term),
                                 score=sum(negative))

word.negative.term.1 <- filter(word.negative.term, term==1)
word.negative.term.1 <- arrange(word.negative.term.1, desc(score))
word.negative.term.2 <- filter(word.negative.term, term==2)
word.negative.term.2 <- arrange(word.negative.term.2, desc(score))

a <- ggplot(word.negative.term.1[1:15,],aes(x=reorder(word, score), y=score)) + 
        geom_bar(stat="identity", fill="midnightblue") +
        xlab("") +
        ylab("Scores") +
        ggtitle("1st Term") +
        coord_flip()

b <- ggplot(word.negative.term.2[1:15,], aes(x=reorder(word, score), y=score)) + 
        geom_bar(stat="identity", fill="firebrick") +
        xlab("") +
        ylab("Scores") +
        ggtitle("2nd Term") +
        coord_flip()

grid.arrange(a, b, nrow=1, ncol=2)
```

Indeed, the comparison of those two graphs show that the conflict lexicon is more present in the second speeches. Of course, that doesn't mean that it is true for all presidents.
I will let to the reader the opportunity to select a president and to display the negative term associated with their first and their second inaugural speeches.

```{r}

ui <- fluidPage(selectInput(inputId = "data","", president.list), 
                plotOutput(outputId = "plot")
)

server <- function(input, output) {
  dfa <- reactive({
    summarise(group_by(dtm.idf.sentiment, word, term, president), score=sum(negative))
  })
  df <- reactive({
    filter(dfa(), president==input$data)
  })
  df1 <- reactive ({
    filter(df(), term==1) %>%
      arrange(desc(score))
  })
  df2 <- reactive ({
    filter(df(), term==2) %>%
      arrange(desc(score))
  })
  # df1 <- reactive ({
  #   arrange(df1(), desc(score))
  # })
  # df2 <- reactive ({
  #   arrange(df2(), desc(score))
  # })

  output$plot <- renderPlot({
    a <- ggplot(df1()[1:15,],aes(x=reorder(word, score), y=score)) + 
        geom_bar(stat="identity", fill="midnightblue") +
        xlab("") +
        ylab("Scores") +
        ggtitle("1st Term") +
        coord_flip()
    b <- ggplot(df2()[1:15,], aes(x=reorder(word, score), y=score)) + 
        geom_bar(stat="identity", fill="firebrick") +
        xlab("") +
        ylab("Scores") +
        ggtitle("2nd Term") +
        coord_flip()
    grid.arrange(a, b, nrow=1, ncol=2)
  }) 
}

shinyApp(ui = ui, server = server, options = list(height = 600))

```


## Step 10 - Emotion Analysis

At at a higher levels we can display the emotions 

```{r}
emotion.score <- summarise(group_by(dtm.idf.sentiment, term, president),
                           anger.score = sum(anger),
                           joy.score = sum(joy),
                           anticipation.score = sum(anticipation),
                           disgust.score = sum(disgust),
                           fear.score = sum(fear),
                           sadness.score = sum(sadness),
                           surprise.score = sum(surprise), 
                           trust.score = sum(trust))
emotion.score <- gather(emotion.score, emotion, score, 
               c(joy.score, anger.score, disgust.score, fear.score, 
                 anticipation.score, sadness.score, surprise.score, sadness.score,
                 trust.score))
emotion.score$emotion <- substr(emotion.score$emotion, 0, nchar(emotion.score$emotion)-6)

emotion.score <- filter(emotion.score, term==c(1,2))
g <- ggplot(emotion.score, aes(x=emotion, y=score, fill=president)) +
  geom_bar(stat="identity", position="stack") +
  coord_flip() +
  xlab("") + 
  facet_wrap(~term, nrow=2)
g
```

## Step 11 - Topic Modeling

```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 15
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
# visualisation
ap_lda <- LDA(dtm, k = 4)
ap_lda

ap_lda_td <- tidy(ldaOut)
ap_lda_td

ap_top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

## Step 12 - Sentiment analysis when going through speeches

```{r}
ggplot.flow(ff.all, "AbrahamLincoln")
ggplot.flow(ff.all, "RichardNixon")
ggplot.flow(ff.all, "GeorgeWBush")
ggplot.flow(ff.all, "RonaldReagan")
ggplot.flow(ff.all, "BarackObama")
```


